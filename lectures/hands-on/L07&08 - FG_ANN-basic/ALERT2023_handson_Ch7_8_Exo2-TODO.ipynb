{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154a3979-b35f-46c7-ab3d-853df3dd4ff0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/alert-geomaterials/2023-doctoral-school/blob/main/lectures/hands-on/L07%2608%20-%20FG_ANN-basic/ALERT2023_handson_Ch7_8_Exo2-TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0820b2c3-5dbc-4355-958b-633da6b9daf1",
   "metadata": {},
   "source": [
    "# **ALERT 2023**\n",
    "\n",
    "## **Artificial Neural Networks - Chapters 7 and 8**\n",
    "\n",
    "\n",
    "Author: Filippo Gatti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-lambda",
   "metadata": {
    "id": "quick-lambda"
   },
   "source": [
    "### Disclaimer\n",
    "\n",
    "This hands-on notebook is devoted to **artificial Neural Networks** ($\\mathcal{NN}$) and it covers chapters 7 and 8.\n",
    "\n",
    "In the following, the code cells introduced by a tag **[TODO]** are meant to be completed by you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5WynSirZMJCG",
   "metadata": {
    "id": "5WynSirZMJCG"
   },
   "outputs": [],
   "source": [
    "# basic packages\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# This is required only for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30Tatj9PQfHG",
   "metadata": {
    "id": "30Tatj9PQfHG"
   },
   "source": [
    "# **Exercise** Design a deep $\\mathcal{LSTM}$-based network to predict earthquake structural response (see Section 2.8, Chapter 7)\n",
    "\n",
    "## **Quick facts**\n",
    "\n",
    "- $\\mathcal{LSTM}$ are widely  used to predict the non-linear transient dynamics of complex physical systems\n",
    "\n",
    "- $\\mathcal{LSTM}$ are powerful input/output surrogate models of the system dynamics, especially if compared with traditional linear and non-linear regressions methods for time-series forcast, such as ARMA, ARIMA, NARMAX etc. Therefore, $\\mathcal{LSTM}$ are largely used to identify complex system's time evolution, based on an external excitation (the $\\mathcal{LSTM}$ input)\n",
    "\n",
    "- Once trained, $\\mathcal{LSTM}$ surrogate cumbersome numerical simulations (such as Finite Element Analysis) since they intrisically learn the system non-linear dynamics and enable the user with a reliable forecast tool\n",
    "\n",
    "- In physics, the traditional dynamic system time evolution is often encapsulated into a second-order Ordinary Differential Equation (ODE) obtained after discretization of the underlying continuous problem. For instance, calling  $\\mathbf{y}(t)\\subset \\mathbb{R}^{p}$ the transient dynamic response of a $p$ Degrees-of-Freedom (DoF) system $\\mathcal{M}$ under dynamic inertial load $\\mathbf{\\ddot x}(t)$ (the dots stand for time derivative), can be encapsulated in the following vector equation:\n",
    "\n",
    "$$ \\mathbf{M}\\mathbf{\\ddot y}(t)+\\mathbf{C}\\mathbf{\\dot y}(t)+\\mathbf{F}(\\mathbf{y}(t),\\mathbf{\\dot y}(t))=- \\mathbf{M}\\mathbf{\\Gamma}\\ddot x(t)$$\n",
    "\n",
    "with $\\mathbf{M}$ and $\\mathbf{C}$ the mass and damping matrices, $\\mathbf{F}(\\mathbf{y}(t),\\mathbf{\\dot y}(t))$ the non-linear restoring force vector and $\\mathbf{\\Gamma}$ the vector of the participation factor.\n",
    "\n",
    "- A nice kindergarten $\\mathcal{LSTM}$ application to predict the non-linear dynamics of a second-order ODE is provided by [1]. The authors solved the non-linear 5-DoF ($p$=5) dynamic equation adopting the  Bouc-Wen model for restoring forces, obtained by solving the following ODE:\n",
    "\n",
    "$$\\dot f_i(t)=k_i\\cdot \\left(\\dot y_{i+1}(t)-\\dot y_i(t)\\right)-\\alpha_i\\cdot\\vert \\dot y_{i+1}(t)-\\dot y_i(t) \\vert \\cdot \\vert f_i(t) \\vert ^{n_i-1}\\cdot f_i(t) -\\beta_i\\cdot \\left(\\dot y_{i+1}(t)-\\dot y_i(t)\\right)\\cdot \\vert f_i(t) \\vert ^{n_i}, i=1,\\ldots,p$$\n",
    "\n",
    "with the following details:\n",
    "\n",
    "-- Fixed base: $ y_0(t)=0$\n",
    "\n",
    "-- $k_i$ stiffness of the $i^{\\text{th}}$ DoF (interstorey stiffness between $i+1^{\\text{th}}$ and $i^{\\text{th}}$ storey)\n",
    "\n",
    "-- Masses: $m_1$=100 kg, $m_2$= $m_3$=$m_4$=$m_5$=80 kg\n",
    "\n",
    "-- Damping coefficients: $c_1$=$c_3$=$c_5$=0.55 kNs/m and $c_2$=$c_4$=0.5 kNs/m\n",
    "\n",
    "-- Interstorey stiffness coefficients: $k_1$=$k_3$=$k_5$=30 kN/m and $k_2$=$k_4$=24 kN/m\n",
    "\n",
    "-- $\\alpha_i$, $\\beta_i$, $n_i$ are the Bouc-Wen non-linear model parameters:\n",
    "  --- $\\alpha_1$=$\\alpha_3$=$\\alpha_5$=1 and $\\alpha_2$=$\\alpha_4$=2\n",
    "\n",
    "  --- $\\beta_1$=$\\beta_3$=$\\beta_5$=2 and $\\beta_2$=$\\beta_4$=1\n",
    "\n",
    "  --- $n_1$=$n_3$=$n_5$=3 and $n_2$=$n_4$=2\n",
    "\n",
    "-- The natural frequencies of the system are:  0.83 Hz, 2.39 Hz, 3.74 Hz, 4.80 Hz, and 5.57 Hz\n",
    "\n",
    "- Data from Bouc-Wen non-linear 5-DoF oscillators can be downloaded here https://www.dropbox.com/sh/xyh9595l79fbaer/AACxiLyKQTAXkujrVSeqsdUsa/data_BoucWen.mat?dl=0\n",
    "\n",
    "- The dataset contains $N$=50  input ground motion accelerations $\\ddot x^{(j)}(t)\\subset\\mathbb{R}$, correspond to random band-limited white noise  ground motion sequences\n",
    "with different magnitudes, as long as the corresponding interstorey drift $\\mathbf{\\Delta y}^{(j)}(t)\\subset\\mathbb{R}^p$. The input ground motions.\n",
    "\n",
    "- 37 isntances are used for training (with relative indices) and 13 for testing  the deep $\\mathcal{LSTM}$ model defined in the following\n",
    "\n",
    "- Moreover, the dataset contains $N_n$=50 samples which are assumed as unknown dataset for testing the predictability performance of the trained model\n",
    "\n",
    "\n",
    "## *Bibliography*\n",
    "[1] Zhang, R.; Chen, Z.; Chen, S.; Zheng, J.; Büyüköztürk, O.; Sun, H. Deep Long Short-Term Memory Networks for Nonlinear Structural Seismic Response Prediction. Computers & Structures 2019, 220, 55-68. https://doi.org/10.1016/j.compstruc.2019.05.006.\n",
    "\n",
    "[2] Zhang, R.; Phillips, B. M.; Taniguchi, S.; Ikenaga, M.; Ikago, K. Shake Table Real‐time Hybrid Simulation Techniques for the Performance Evaluation of Buildings with Inter-story Isolation. Struct. Control Health Monit. 2017, 24 (10). https://doi.org/10.1002/stc.1971.\n",
    "\n",
    "[3] Wen, Y.-K. Method for Random Vibration of Hysteretic Systems. Journal of the Engineering Mechanics Division 1976, 102 (2), 249-263. https://doi.org/10.1061/JMCEA3.0002106."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dhWKhZ482_ev",
   "metadata": {
    "id": "dhWKhZ482_ev"
   },
   "source": [
    "## **Questions**\n",
    "\n",
    "- Can you reproduce the 5-DoF response with a deep $\\mathcal{LSTM}$ model?\n",
    "\n",
    "- How many $\\mathcal{LSTM}$ layers should be considered?\n",
    "\n",
    "- Is it reasonable to stack one or more $\\mathcal{MLP}$ at the top of the $\\mathcal{LSTM}$ layer(s)? Why?\n",
    "\n",
    "## **Learning Outcomes**\n",
    "\n",
    "- Learn how to design a deep $\\mathcal{LSTM}$ to predit the dynamic response of $p$-DoF\n",
    "\n",
    "- Learn how to combine $\\mathcal{LSTM}$ and $\\mathcal{MLP}$ layers to connect the $\\mathcal{LSTM}$ layers to the target output layer and thus construct the desired\n",
    "number of output features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YVklgF-u5Hr_",
   "metadata": {
    "id": "YVklgF-u5Hr_"
   },
   "source": [
    "## **Objective**\n",
    "\n",
    "In the following, you are asked to conceive a deep $\\mathcal{LSTM}$ network and train it on the database provided by [1] (described above), referring to the seismic response (inter-storey drift, output of the network) of a 5-DoF Bouc-Wen non-linear multi-storey building under earthquake ground motion acceleration (input of the network)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XJsuFepB8O3H",
   "metadata": {
    "id": "XJsuFepB8O3H"
   },
   "source": [
    "## - Step 1: Download the database provided by [1] and parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OO_Hvt4FQX0Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6144,
     "status": "ok",
     "timestamp": 1693405012276,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "OO_Hvt4FQX0Y",
    "outputId": "8fec3dd8-a24d-42b3-fa6c-7255d5e9ad98"
   },
   "outputs": [],
   "source": [
    "!wget -O data_BoucWen.mat https://www.dropbox.com/sh/xyh9595l79fbaer/AACxiLyKQTAXkujrVSeqsdUsa/data_BoucWen.mat?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ZJyA44d-zfR",
   "metadata": {
    "id": "7ZJyA44d-zfR"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "import scipy.io\n",
    "data = scipy.io.loadmat('data_BoucWen.mat')\n",
    "\n",
    "# Training/Testing data\n",
    "X = data['input_tf']  # input ground motion acceleration\n",
    "y = data['target_tf'] # 5 inter-storey drifts\n",
    "\n",
    "# Unknown data\n",
    "X_pred = data['input_pred_tf'] # input ground motion acceleration\n",
    "y_pred_ref = data['target_pred_tf'] # 5 inter-storey drifts\n",
    "\n",
    "# Training/testing indices\n",
    "train_indices = data['trainInd'] - 1 # indices for train set\n",
    "test_indices = data['valInd'] - 1 # indices for test set\n",
    "\n",
    "# Number of samples N, number of time steps nt, number of\n",
    "(N, nt, p) = y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lt7DYlxn-SDn",
   "metadata": {
    "id": "lt7DYlxn-SDn"
   },
   "source": [
    "## - Step 2 **[TODO]**: Scale and subsample (factor 10) the database. Split `torch` tensors in train/test databases\n",
    "\n",
    "*Note*: use the same scaler for `X` and `X_pred` and the same scaler for `y` and `y_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kk8nIMZeWK_A",
   "metadata": {
    "id": "Kk8nIMZeWK_A"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Subsampling factor\n",
    "ss = 10\n",
    "\n",
    "# Scale data X\n",
    "X = X.flatten()[:,np.newaxis]\n",
    "\n",
    "# Hint: use scikit-learn MinMaxScaler\n",
    "Xscaler = # [TODO]\n",
    "Xscaler.fit(X)\n",
    "Xs = Xscaler.transform(X)\n",
    "Xs = np.reshape(Xs, [N, nt, 1])[:,0::ss,:]\n",
    "\n",
    "# Scale data y\n",
    "y = np.reshape(y, [N*nt, p])\n",
    "yscaler =  # [TODO]\n",
    "yscaler.fit(y)\n",
    "ys = yscaler.transform(y)\n",
    "ys = np.reshape(ys, [N, nt, p])[:,0::ss,:]\n",
    "\n",
    "# Scale data X predicted\n",
    "X_pred = X_pred.flatten()[:,np.newaxis]\n",
    "Xs_pred =  # [TODO]\n",
    "Xs_pred = np.reshape(Xs_pred, [N, nt, 1])[:,0::ss,:]\n",
    "\n",
    "# Scale data y predicted\n",
    "y_pred = np.reshape(y_pred_ref, [N*nt, p])\n",
    "ys_pred =  # [TODO]\n",
    "ys_pred = np.reshape(ys_pred, [N, nt, p])[:,0::ss,:]\n",
    "\n",
    "# Train dataset\n",
    "X_train = torch.tensor(Xs[0:len(train_indices[0]),:]).to(torch.float32)\n",
    "y_train = torch.tensor(ys[0:len(train_indices[0]),:,:]).to(torch.float32)\n",
    "\n",
    "# Test dataset\n",
    "X_test = torch.tensor(Xs[len(train_indices[0]):,:]).to(torch.float32)\n",
    "y_test = torch.tensor(ys[len(train_indices[0]):,:,:]).to(torch.float32)\n",
    "\n",
    "# Train dataset\n",
    "X_train = torch.tensor(Xs[0:len(train_indices[0]),:]).to(torch.float32)\n",
    "y_train = torch.tensor(ys[0:len(train_indices[0]),:,:]).to(torch.float32)\n",
    "\n",
    "# Prediction dataset\n",
    "X_pred = torch.tensor(Xs_pred).to(torch.float32)\n",
    "y_pred = torch.tensor(ys_pred).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YuATuMRS2gtt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "executionInfo": {
     "elapsed": 2170,
     "status": "ok",
     "timestamp": 1693405463562,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "YuATuMRS2gtt",
    "outputId": "c5008083-6344-4e5e-92d4-40d5b89d4c1f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, data[\"input_tf\"].shape[0]))\n",
    "for i, (yp, c) in enumerate(zip(data[\"input_tf\"], colors)):\n",
    "    ax.plot(yp/20+i, linewidth=0.5, color=c)\n",
    "ax.set_xlabel(\"$t [s]$\")\n",
    "ax.set_ylabel(r\"$x_g(t)$\")\n",
    "ax.set_yticks([])\n",
    "ax.set_title('Input ground motion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xh1yhba9_cPH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1eQtAd4hMcQbYx1A-WF00H1O11ireOklP"
    },
    "executionInfo": {
     "elapsed": 9863,
     "status": "ok",
     "timestamp": 1693405476381,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "xh1yhba9_cPH",
    "outputId": "f11823bf-dd05-4a3a-cfcf-0bb0dc67d55e"
   },
   "outputs": [],
   "source": [
    "colors = cm.rainbow(np.linspace(0, 1, data[\"target_tf\"].shape[0]))\n",
    "\n",
    "for p in range(data[\"target_tf\"].shape[-1]):\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    for i, (yp, c) in enumerate(zip(data[\"target_tf\"][:,:,p], colors)):\n",
    "        ax.plot(yp+i/10, linewidth=0.5, color=c)\n",
    "    ax.set_xlabel(\"$t [s]$\")\n",
    "    ax.set_ylabel(r\"$x_g(t)$\")\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(r\"$y_{{{:d}}}$\".format(p+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YN8h6RBAAtMM",
   "metadata": {
    "id": "YN8h6RBAAtMM"
   },
   "source": [
    "# - Step 3 **[TODO]**: design a deep $\\mathcal{LSTM}$\n",
    "\n",
    "See the `pytorch` documentation https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "The final $\\mathcal{NN]$ should consist in:\n",
    "\n",
    "-- one initial `nn.LSTM` with  $fan\\_out$=50 with `nn.ReLU` activation function\n",
    "\n",
    "-- one second `nn.LSTM` with $fan\\_out$=50 with `nn.ReLU`activation function\n",
    "\n",
    "-- two `Linear` layers with $fan\\_in=fan\\_out$=50\n",
    "\n",
    "-- 20% dropout in the `nn.LSTM` layers\n",
    "\n",
    "*Hint*: Design the custom deep $\\mathcal{LSTM}$ network by defining a subclass of `torch.nn.Module` class, in order to customize its `forward` method\n",
    "\n",
    "*Note*:\n",
    "the `pytorch` $\\mathcal{LSTM}$ implementation (`LSTM`) takes as input a time history tensor `X` with shape $t, f$, whre $t$ indicates the time length and $f$ the number of features.\n",
    "\n",
    "The first $\\mathcal{LSTM}$ takes all the $N$ training time histories $\\ddot{x}^{(j)}(t)$ input ground motion accelerations, sampled at $nt$ time-steps, stacked and stored in the `X_train` variable, whose shape is `X_train.shape`=$[N,nt,1]$. In this case, do not forget to use the option `batch_first=True`\n",
    "\n",
    "*Note*:\n",
    "the `pytorch` $\\mathcal{LSTM}$ implementation (`LSTM`) outputs the hidden state $\\mathbf{h}(t)$, whose time size $fan\\_out$ corresponds to the input variable `hidden_size`. Moreover, `LSTM` provides the last update of the cell and hidden state $\\mathbf{c}(t_{fan\\_out})$, $\\mathbf{h}(t_{fan\\_out})$. Therefore,  `nn.LSTM` cannot used in the `nn.Sequential` wrapper.\n",
    "\n",
    "*Note*:\n",
    "the full hidden state outputted by the first `LSTM` layer is the input of the second `LSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lDI7obIqgqOf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1693405031761,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "lDI7obIqgqOf",
    "outputId": "2ccf263f-2f0d-4895-9a4e-766335ef518d"
   },
   "outputs": [],
   "source": [
    "'''[TODO] Design a custom deep LSTM model'''\n",
    "from torch import nn\n",
    "fan_out1 = 50\n",
    "fan_out2 = 50\n",
    "Nl = 2\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, fan_outs, out_features):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1st layer LSTM\n",
    "        self.lstm1 = nn.LSTM(input_size=1,\n",
    "            hidden_size=fan_outs[0],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0.0)\n",
    "\n",
    "        # 2nd LSTM layer\n",
    "        self.lstm2 = nn.LSTM(input_size=fan_outs[0],\n",
    "            hidden_size=fan_outs[1],\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0.0)\n",
    "\n",
    "        # 1st layer MLP\n",
    "        self.mlp1 = nn.Linear(in_features = fan_outs[1],\n",
    "                              out_features =fan_outs[1])\n",
    "        # 2nd layer MLP\n",
    "        self.mlp2 = nn.Linear(in_features =fan_outs[1],\n",
    "                            out_features = out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1, (hn1, cn1) = self.lstm1(x)  # lstm (1)\n",
    "        h1 = nn.functional.relu(h1) # relu (1)\n",
    "        h1 =  # [TODO] add dropout\n",
    "        h2, (hn2, cn2)=  # [TODO] add lstm (2)\n",
    "        h2 =  # [TODO] add relu (2)\n",
    "        h2 = nn.functional.dropout(h2)\n",
    "        h3 = # [TODO] add linear (1)\n",
    "        y =  # [TODO] add final output layer\n",
    "        return y\n",
    "\n",
    "h_theta = CustomLSTM([fan_out1, fan_out2], out_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TePbTfCNGTJD",
   "metadata": {
    "id": "TePbTfCNGTJD"
   },
   "source": [
    "## - Step 4: choose the optimization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-_Epo8PBDf2f",
   "metadata": {
    "id": "-_Epo8PBDf2f"
   },
   "outputs": [],
   "source": [
    "'''Optimization setup'''\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# number of epochs\n",
    "n_e = 500\n",
    "\n",
    "# batch size and batch per epochs\n",
    "batch_size = 10\n",
    "batches_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.001\n",
    "beta1 = 0.9 # Adam coefficients\n",
    "beta2 = 0.999 # Adam coefficients\n",
    "epsilon = 1e-6 # tolerance\n",
    "optimizer = torch.optim.AdamW(h_theta.parameters(),\n",
    "                             lr=learning_rate,\n",
    "                             betas=(beta1, beta2),\n",
    "                             eps=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9DoClrkwMZdA",
   "metadata": {
    "id": "9DoClrkwMZdA"
   },
   "source": [
    "## - Step 5 **[TODO]** train the deep $\\mathcal{NN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vo1ZjHXq3UDE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57825,
     "status": "ok",
     "timestamp": 1693405089565,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "vo1ZjHXq3UDE",
    "outputId": "b6c88ffa-8cf3-445c-c6b8-dc40f9222626"
   },
   "outputs": [],
   "source": [
    "'''[TODO] Train the NN'''\n",
    "import tqdm\n",
    "train_loss_hist = []\n",
    "test_loss_hist = []\n",
    "for epoch in range(n_e):\n",
    "    epoch_loss = []\n",
    "    # set model in training mode\n",
    "    h_theta.train()\n",
    "\n",
    "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for i in bar:\n",
    "            # take a batch\n",
    "            start = i * batch_size\n",
    "            X_batch = X_train[start:start+batch_size,:,:]\n",
    "            y_batch = y_train[start:start+batch_size,:,:]\n",
    "            idx = torch.randperm(X_batch.shape[0])\n",
    "            X_batch = X_batch[idx,:,:]\n",
    "            y_batch = y_batch[idx,:,:]\n",
    "            # infer (forward)\n",
    "            # [TODO]\n",
    "            # compute the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # reset previously saved gradients and empty the optimizer memory\n",
    "            optimizer.zero_grad()\n",
    "            # run backward propagation\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # compute and store metrics\n",
    "            epoch_loss.append(float(loss))\n",
    "\n",
    "    # set model in evaluation mode to infer the class in the test set\n",
    "    # without storing gradients for brackprop\n",
    "    h_theta.eval()\n",
    "    # infer the class over the test set\n",
    "    y_pred = h_theta(X_test)\n",
    "    acc = float(loss_fn(y_pred, y_test))\n",
    "    train_loss_hist.append(np.mean(epoch_loss))\n",
    "    test_loss_hist.append(acc)\n",
    "    # print(f\"Epoch {epoch} validation: MSE={acc:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfUOAQsbMrif",
   "metadata": {
    "id": "bfUOAQsbMrif"
   },
   "source": [
    "## - Step 5 **[TODO]**: plot the learning curves to assess the goodness of the learning process. Use the learning curve to fine tune the custom deep $\\mathcal{LSTM}$ network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u0JXoV6fQyc-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 1445,
     "status": "ok",
     "timestamp": 1693405318792,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "u0JXoV6fQyc-",
    "outputId": "8c226719-8a41-4323-d42a-7947580adef2"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.plot(\n",
    "    # [TODO]\n",
    "            label=r\"train\")\n",
    "ax.plot(\n",
    "     # [TODO]\n",
    "            label=r\"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$\\int_{0}^T\\Vert \\mathbf{y}(t)-\\mathbf{h}_{\\mathbf{\\theta}}(t) \\Vert^2 dt$\")\n",
    "ax.set_xlim(0,n_e)\n",
    "# ax.set_ylim(1e-6,1e0)\n",
    "ax.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, 1.0),ncol=2,)\n",
    "fig.savefig(\"mse_fx_compare.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JBfpJYlFM_Db",
   "metadata": {
    "id": "JBfpJYlFM_Db"
   },
   "source": [
    "## - Step 6 **[TODO]**: compare the $\\mathcal{NN}$ outcome with the dataset for blind prediction. How to improve the $\\mathcal{NN}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hek7bGp0z1xH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "executionInfo": {
     "elapsed": 1216,
     "status": "ok",
     "timestamp": 1693405480632,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "Hek7bGp0z1xH",
    "outputId": "b19936a1-8651-4ce4-d747-a79ecdb5eec3"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(y_pred[0,:,0].detach().cpu().numpy())\n",
    "ax.plot(h_theta(X_pred)[0,:,0].detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
