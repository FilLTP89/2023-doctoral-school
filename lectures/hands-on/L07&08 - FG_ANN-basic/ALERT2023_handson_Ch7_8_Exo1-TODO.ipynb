{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alert-geomaterials/2023-doctoral-school/blob/main/lectures/hands-on/L07%2608%20-%20FG_ANN-basic/ALERT2023_handson_Ch7_8_Exo1-TODO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvfMXRY5jx5M"
   },
   "source": [
    "# **ALERT 2023**\n",
    "\n",
    "## **Artificial Neural Networks - Chapters 7 and 8**\n",
    "\n",
    "\n",
    "Author: Filippo Gatti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrRtpSJ_eLpV"
   },
   "source": [
    "### Disclaimer\n",
    "\n",
    "This hands-on notebook is devoted to **artificial Neural Networks** ($\\mathcal{NN}$) and it covers chapters 7 and 8.\n",
    "\n",
    "In the following, the code cells introduced by a tag **[TODO]** are meant to be completed by you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1693410798576,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "rVRiwgPxqEl7"
   },
   "outputs": [],
   "source": [
    "# basic packages\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# This is required only for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zzfhscvzu4TW"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8bJbXG7miOv"
   },
   "source": [
    "## **Preliminaries**: practicing with `python3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRbjh8FDlrwx"
   },
   "source": [
    "Before getting started, a few general hints:\n",
    "\n",
    "1. `lambda` constructors: they are used in `python` to define anonymous functions ([see this link](https://realpython.com/python-lambda/))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1693410798576,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "ZCYW6lnX0rRV",
    "outputId": "11b0f2aa-ceec-4c9b-ed47-5551a23bcfee"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([1.5, 2.8, 3.1])\n",
    "scale = lambda x: x * 3\n",
    "scale(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlwVHKC4QCgC"
   },
   "source": [
    "2. Handle `pandas` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1693410798912,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "FPA9JSHJQUdn",
    "outputId": "9e833eb7-45e6-440f-bb34-9fce93a74e83"
   },
   "outputs": [],
   "source": [
    "\n",
    "Image(url=\"https://www.tutorialspoint.com/python_pandas/images/structure_table.jpg\",width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1693410798913,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "ta5_ta5iQefI",
    "outputId": "a37b882e-5a24-4d2b-d5b1-14ed3413c8c8"
   },
   "outputs": [],
   "source": [
    "# Empty dataframe\n",
    "df = pd.DataFrame()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1693410798913,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "Zseq-qWdQgEJ",
    "outputId": "ee913f3a-b93c-4a4a-811e-f689b9a3bf0d"
   },
   "outputs": [],
   "source": [
    "# Basic database\n",
    "data = [1,2,3,4,5]\n",
    "df = pd.DataFrame(data)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1693410798913,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "pfzt1ZFAQkyd",
    "outputId": "956bc3b4-9485-4185-97fe-6e08c4d503ff"
   },
   "outputs": [],
   "source": [
    "# Database with column labels\n",
    "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
    "df = pd.DataFrame(data,columns=['Name','Age'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1693410798914,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "rnHJWEViQwCW",
    "outputId": "48a5913c-58c1-43a1-c7a3-9fc006edf023"
   },
   "outputs": [],
   "source": [
    "# Repartitioning data into dataframe\n",
    "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
    "df = pd.DataFrame(data,columns=['Name','Age'],dtype=float)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1693410798914,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "sI3X_pS6Qwjp",
    "outputId": "25a62abd-1461-48b8-9479-fdb444d92993"
   },
   "outputs": [],
   "source": [
    "# Deal with indices\n",
    "data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}\n",
    "df = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXEd3Abh3dxD"
   },
   "source": [
    "3. plot data: ([see this link](https://matplotlib.org/tutorials/introductory/usage.html#sphx-glr-tutorials-introductory-usage-py))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1693410799398,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "fjLz5ga_8crQ",
    "outputId": "67f470fc-9e83-40bb-8386-5800408681ed"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 100)\n",
    "plt.plot(x, x, label='linear')\n",
    "plt.plot(x, x**2, label='quadratic')\n",
    "plt.plot(x, x**3, label='cubic')\n",
    "plt.xlabel('x label')\n",
    "plt.ylabel('y label')\n",
    "plt.title(\"Simple Plot\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqdFye6vWfYM"
   },
   "source": [
    "# **Exercise 1** Non-linear regressions of real functions with $\\mathcal{MLP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHcUGwPMbAwT"
   },
   "source": [
    "The aim of this task is to get acquainted with non-linear polynomial regressions. In this task, the following *kindergarten* equation is considered:\n",
    "\n",
    "$$f(x) = \\sin(30 \\cdot (x-0.9)^4)\\cos(2 \\cdot (x-0.9))+\\frac{x-0.9}{2}$$\n",
    "\n",
    "Given the equation above, solve the following issues:\n",
    "\n",
    "## - Step 1 **[TODO]**: Plot $f(x)$ and evaluate it at $N$=100 random points $x_i\\sim\\mathcal{U}\\left[0,1\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "executionInfo": {
     "elapsed": 798,
     "status": "ok",
     "timestamp": 1693410800191,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "SsO6NkXVoIsV",
    "outputId": "9c93f27c-5c6d-4a65-ed0e-baa1a5a82bf4"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "# Grant reproductibility\n",
    "np.random.seed(0)\n",
    "N = 10\n",
    "x_plot = np.linspace(0, 1, N*100)\n",
    "rng = np.random.RandomState(0)\n",
    "x_train = np.sort(rng.choice(x_plot,\n",
    "                             size=N,\n",
    "                             replace=False))\n",
    "def f(x):\n",
    "    return # [TODO]\n",
    "y_train = f(x_train)\n",
    "\n",
    "# create 2D-array versions of these arrays to feed to transformers\n",
    "X_train = x_train[:, np.newaxis]\n",
    "X_plot = x_plot[:, np.newaxis]\n",
    "\n",
    "# plot function\n",
    "lw = 2\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(\n",
    "    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n",
    ")\n",
    "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
    "\n",
    "# plot training points\n",
    "ax.scatter(x_train, y_train, label=\"training points\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"f(x)\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihkjOEf_cLvW"
   },
   "source": [
    "\n",
    "\n",
    "## - Step 2 **[TODO]**: Fit the selected points for different polynomial orders (hint: 3,4,5,... or piece-wise polynomials). Show the fitting improvements obtained when changing the polynomial order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 1372,
     "status": "ok",
     "timestamp": 1693410801553,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "lz6aBTurhLIX",
    "outputId": "1fb119c6-0393-4a96-be94-89a9bfedb228"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, SplineTransformer\n",
    "\n",
    "lw = 2\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(\n",
    "    color=[\"black\", \"teal\", \"yellowgreen\",\n",
    "           \"gold\", \"darkorange\", \"tomato\",\n",
    "           \"skyblue\"]\n",
    ")\n",
    "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
    "\n",
    "# plot training points\n",
    "ax.scatter(x_train, y_train, label=\"training points\")\n",
    "\n",
    "# polynomial features\n",
    "alpha = 1e-10# penalty coefficient\n",
    "for degree in [3, 4, 5, 9, N]:\n",
    "    model = make_pipeline(PolynomialFeatures(degree),\n",
    "                          Ridge(alpha=alpha))\n",
    "    # [TODO]\n",
    "    # Hint: use the sklearn function `fit` and `predict`\n",
    "    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73L6MJjZhKU9"
   },
   "source": [
    "\n",
    "## - Step 3 **[TODO]**: How the fit improves when considering 100, 1000 random points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 2767,
     "status": "ok",
     "timestamp": 1693410804318,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "dxKo-1bsoy_l",
    "outputId": "4f64c4af-3083-489e-fef6-3452f50c40e0"
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x_train = np.sort(rng.choice(x_plot,\n",
    "                             size=N,\n",
    "                             replace=False))\n",
    "y_train = f(x_train)\n",
    "X_train = x_train[:, np.newaxis]\n",
    "\n",
    "lw = 2\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(\n",
    "    color=[\"black\", \"teal\", \"yellowgreen\",\n",
    "           \"gold\", \"darkorange\", \"tomato\",\n",
    "           \"skyblue\"]\n",
    ")\n",
    "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
    "\n",
    "# plot training points\n",
    "ax.scatter(x_train, y_train, label=\"training points\")\n",
    "# polynomial features\n",
    "alpha =0 # penalty coefficient\n",
    "for degree in [3, 4, 5, 9, N//2, N-1]:\n",
    "    model = make_pipeline(PolynomialFeatures(degree),\n",
    "                          Ridge(alpha=alpha))\n",
    "    # [TODO]\n",
    "    # Hint: use the sklearn function `fit` and `predict`\n",
    "    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bv66hKpozO-"
   },
   "source": [
    "## - Step 4 **[TODO]**:  Design a $\\mathcal{MLP}$ to fit the curve sampled with 10, 100, 1000 points respectively. Use the least number of layers and neurons possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWM9YcBFHTS5"
   },
   "source": [
    "## - Step 4.1 **[TODO]**:  create and split dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6805,
     "status": "ok",
     "timestamp": 1693410811121,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "DmvXrMPbHO8m",
    "outputId": "d5965fe2-3b5c-4b4f-fff7-8402df030392"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1693410811122,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "mAEE1-ops902"
   },
   "outputs": [],
   "source": [
    "'''[TODO] Create dataset'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "m = torch.distributions.Uniform(torch.zeros(1),\n",
    "                                torch.ones(1))\n",
    "\n",
    "N = 1000\n",
    "X = m.sample((N,)).to(torch.float32)\n",
    "\n",
    "# split into train and test sets\n",
    "# Hint: use the sklearn function `train_test_split`\n",
    "# [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHll11ZbI4U7"
   },
   "source": [
    "## - Step 4.2 **[TODO]**:  design the $\\mathcal{MLP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1693410811123,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "9OvUF4A_8_ta"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "'''[TODO] Design MLP'''\n",
    "\n",
    "fan_in = 4\n",
    "h_theta = nn.Sequential(\n",
    "    # [TODO]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyVUTJR8JBvN"
   },
   "source": [
    "## - Step 4.3 **[TODO]**:  choose the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1693410811488,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "WXtCCbeSwJCC"
   },
   "outputs": [],
   "source": [
    "'''[TODO] Optimization setup'''\n",
    "# Define the loss function\n",
    "loss_fn = # [TODO]\n",
    "\n",
    "# number of epochs\n",
    "n_e = 800\n",
    "\n",
    "# batch size and batch per epochs\n",
    "batch_size = 200\n",
    "batches_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.0001\n",
    "beta1 = 0.9 # Adam coefficients\n",
    "beta2 = 0.999 # Adam coefficients\n",
    "epsilon = 1e-8 # tolerance\n",
    "optimizer = # [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2x-Z8NuJNDm"
   },
   "source": [
    "## - Step 4.4 **[TODO]**:  train the $\\mathcal{MLP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62483,
     "status": "ok",
     "timestamp": 1693410873964,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "qFQeWdXIv9Hv",
    "outputId": "2ad84eb0-3562-4769-fc10-dd79813f9898"
   },
   "outputs": [],
   "source": [
    "'''[TODO] Train the hidden-layer MLP'''\n",
    "import tqdm\n",
    "train_loss_hist = []\n",
    "test_loss_hist = []\n",
    "for epoch in range(n_e):\n",
    "    epoch_loss = []\n",
    "    # set model in training mode\n",
    "    h_theta.train()\n",
    "\n",
    "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for i in bar:\n",
    "            # take a batch\n",
    "            start = i * batch_size\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # infer (forward)\n",
    "            y_pred = h_theta(X_batch).squeeze()\n",
    "            # compute the loss\n",
    "            loss = # [TODO]\n",
    "            # reset previously saved gradients and empty the optimizer memory\n",
    "            optimizer.zero_grad()\n",
    "            # run backward propagation\n",
    "            # [TODO]\n",
    "            # update weights\n",
    "            # [TODO]\n",
    "            # compute and store metrics\n",
    "            epoch_loss.append(float(loss))\n",
    "\n",
    "    # set model in evaluation mode to infer the class in the test set\n",
    "    # without storing gradients for brackprop\n",
    "    h_theta.eval()\n",
    "    # infer the class over the test set\n",
    "    y_pred = h_theta(X_test).squeeze()\n",
    "    acc = float(loss_fn(y_pred, y_test))\n",
    "    train_loss_hist.append(np.mean(epoch_loss))\n",
    "    test_loss_hist.append(acc)\n",
    "    # print(f\"Epoch {epoch} validation: MSE={acc:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WHopbt0JR7L"
   },
   "source": [
    "## - Step 4.5 **[TODO]**:  plot the learning curves and the comparison between test data and prediction from trained $\\mathcal{MLP}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "executionInfo": {
     "elapsed": 84,
     "status": "ok",
     "timestamp": 1693410873965,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "NaNtL74y14Oy",
    "outputId": "c4fa1ae8-3a61-42dc-e103-c1226ca2f2f7"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.semilogy(train_loss_hist,\n",
    "            color='b',\n",
    "            linewidth=1,\n",
    "            label=r\"train $h_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
    "# [TODO] Plot test curve (use label=r\"test $h_{\\mathbf{\\theta}}(\\mathbf{x})$\"))\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$\\Vert f(x)-h_\\theta(x) \\Vert^2$\")\n",
    "ax.set_xlim(0,n_e)\n",
    "ax.set_ylim(1e-6,1e0)\n",
    "ax.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, 1.0),ncol=2,)\n",
    "fig.savefig(\"mse_fx_compare.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1693410873965,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "VDEVttiz21hw",
    "outputId": "be32491a-52b8-4a2d-8daa-8206d1f7a711"
   },
   "outputs": [],
   "source": [
    "h_theta.eval()\n",
    "\n",
    "lw = 2\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_prop_cycle(\n",
    "    color=[\"black\", \"teal\", \"yellowgreen\",\n",
    "           \"gold\", \"darkorange\", \"tomato\",\n",
    "           \"skyblue\"]\n",
    ")\n",
    "ax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n",
    "\n",
    "ax.scatter(X_test.cpu().numpy(),h_theta(X_test).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NR051wadwn7C"
   },
   "source": [
    "# **Exercise 2**: Approximate an approximately-radial function with $\\mathcal{MLP}$ (see Section 2.4, Chapter 7)\n",
    "\n",
    "## **Quick facts**\n",
    "\n",
    "\n",
    "- Despite the fact that Universal approximation theorem for a 1-hidden-layered perceptron (see Theorem 24, Chapter 8) proves the universal approximation capability of a 1-hidden-layer $\\mathcal{MLP}$, provided that enough neurons are considered, this result is quite hard to exploit in real applications, since the number of neurons can easily become too large to handle from a computational standpoint.\n",
    "\n",
    "- Eldan and Shamir [1] showed that it exists an approximately radial function $\\varphi(\\mathbf{x}):\\mathbb{R}^{d_X}\\to\\mathbb{R}$, $\\varphi: \\mathbf{x}\\mapsto\\varphi(\\Vert\\mathbf{x}\\Vert)$ that can be approximated by a ``small`` (bounded number of neurons) 2-hidden-layers $\\mathcal{MLP}$ with arbitrary accuracy, but that cannot be approximated by a 1-hidden-layer $\\mathcal{MLP}$ below a certain accuracy, unless the number of neurons $N_K$ grows exponentially with $d_X$.\n",
    "\n",
    "- In particular, this results is valid for any activation function $g$ and with no further constraint on the weights and biases adopted in the $\\mathcal{MLP}$ (on the contrary, the Universal Approximation Theorem requires that the high-frequency components $\\Vert \\mathbf{w}_n\\Vert$ are smaller than a constant).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### *Bibliography*:\n",
    "\n",
    "[1] Eldan, R.; Shamir, O. The Power of Depth for Feedforward Neural Networks. In Workshop and Conference Proceedings; PMLR, 2016; Vol. 49, pp 1--34. url: https://proceedings.mlr.press/v49/eldan16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j1SBUCH-AZ7"
   },
   "source": [
    "## **Questions**\n",
    "\n",
    "- What kind of functions cannot be approximated with a *reasonable* accuracy by a $\\mathcal{MLP}$ of $N_\\ell$ layers?\n",
    "\n",
    "- How many neurons should be considered for each layer?\n",
    "\n",
    "- What is the effect of having a number of layers that is higher than the number of neurons per layer?\n",
    "\n",
    "## **Learning Outcomes**\n",
    "\n",
    "- This results proves that increasing the depth of the $\\mathcal{MLP}$ widens the approximation capability of the $\\mathcal{MLP}$ and that the depth of the $\\mathcal{MLP}$ should be privileged with the respect to its layer dimension (but being careful to avoid vanishing\n",
    "gradient problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ny6N7jz-HOs"
   },
   "source": [
    "## **Objective**\n",
    "\n",
    "In the following, you are asked to conceive a multi-layer feed-forward $\\mathcal{MLP}$ to approximate the following approximately-radial function and compare the accuracy obtained with 1-hidden layer and with 2-hidden layers. The approximately-radial function is defined as:\n",
    "\n",
    "$$\\varphi(\\mathbf{x}) = \\left(\\frac{R_{d_X}}{\\Vert\\mathbf{x}\\Vert}\\right)^{\\frac{d_X}{2}}J_{\\frac{d_X}{2}}(2\\pi R_{d_X}\\Vert\\mathbf{x}\\Vert)  =\\int_{\\mathbf{w}:\\Vert\\mathbf{w}\\Vert\\le R_d} e^{-2\\pi i \\langle \\mathbf{x}, \\mathbf{w}\\rangle} d\\mathbf{w}\\phantom{}^{(1)}\n",
    "$$\n",
    "\n",
    "with $J_{\\frac{d_X}{2}}(2\\pi R_{d_X}\\Vert\\mathbf{x}\\Vert)$ the Bessel function of first kind of order $\\frac{d_X}{2}$. In the following, consider $d_X=2$.\n",
    "\n",
    "**$\\phantom{}^{(1)}$Note**: The approximately radial function is the inverse Fourier transform of the indicator function on a unit volume euclidean ball $B_{R_{d_X}}(0)$, of radius $R_{d_X}$ such that its volume $V_{d_X}(R_{d_X}) = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuepuQKsCv8L"
   },
   "source": [
    "## - Step 1: compute and plot the approximately-radial function $\\varphi$ on, restricted to a compact set $\\mathcal{X}_{\\square}=\\left[-l_x,l_x\\right]\\times\\left[-l_y,l_y\\right]\\subset\\mathbb{R}^2$ over a discrete regular grid of $n_x\\times n_y$ points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1693410873965,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "7LYWqFGgksDx"
   },
   "outputs": [],
   "source": [
    "'''Define grid in R2'''\n",
    "lx = 10.0\n",
    "ly = 10.0\n",
    "x_lim = np.array([-lx,lx], dtype=np.float64) # x-bounds\n",
    "y_lim = np.array([-ly,ly], dtype=np.float64) # y-bounds\n",
    "\n",
    "nx = 1001 # number of grid points along x\n",
    "ny = 1001 # number of grid points along y\n",
    "# vector of coordinates along x\n",
    "xv = np.linspace(x_lim[0], x_lim[1], nx, endpoint=True)\n",
    "# vector of coordinates along y\n",
    "yv = np.linspace(y_lim[0], y_lim[1], ny, endpoint=True)\n",
    "# define mesh grid\n",
    "yg, xg = np.meshgrid(yv,xv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yjldGWsEwvP"
   },
   "source": [
    "## - Step 2: evaluate $\\varphi$ over the grid and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1693410873966,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "RjNevvVGks5P",
    "outputId": "32c3718b-61bb-4c1a-d072-e0e083b9a5ac"
   },
   "outputs": [],
   "source": [
    "'''Define varphi(x,y) over the grid'''\n",
    "from scipy.special import jv # Bessel function\n",
    "d_X = 2 # dimension of the problem\n",
    "R_d = 0.2 # radius\n",
    "phi = (R_d/(np.sqrt(xg**2+yg**2)))**(int(d_X//2))*jv(int(d_X//2),\n",
    "                                                     2.0*np.pi*R_d*np.sqrt(xg**2+\n",
    "                                                                           yg**2))\n",
    "fig = plt.figure(figsize=(4,4),)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(xg, yg, phi, cmap=cm.jet)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_zlabel(r\"$\\varphi(x,y)$\")\n",
    "fig.savefig(\"radial_function.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKUzqPK9Gsxz"
   },
   "source": [
    "## - Step 3 **[TODO]**: create the training and test datasets by uniformly sampling the function $\\varphi$ over $\\mathcal{X}_\\square$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1693410873966,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "NefRB4EZk0s_"
   },
   "outputs": [],
   "source": [
    "''' [TODO] Create uniformly random grid of points for training'''\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Grant reproductibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Hint: use the package torch.distributions\n",
    "# https://pytorch.org/docs/stable/distributions.html\n",
    "# for uniform distribution\n",
    "m = torch.distributions # [TODO]\n",
    "X = m.sample((4000,)).to(torch.float32)\n",
    "y = (R_d/(np.sqrt(X[:,0]**2+X[:,1]**2)))**(int(d_X//2))*jv(int(d_X//2),\n",
    "                                                           2.0*np.pi*R_d*np.sqrt(\n",
    "                                                              X[:,0]**2+\n",
    "                                                              X[:,1]**2))\n",
    "\n",
    "# split into train and test sets\n",
    "# Hint: use the sklearn function `train_test_split`\n",
    " # [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1693410873966,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "yAw0obKNKwpZ",
    "outputId": "0b73e0ae-998b-4e53-cce4-e43d132c45b3"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,4),)\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.plot_surface(xg, yg, phi, cmap=cm.jet, alpha=0.3)\n",
    "ax.scatter(X_train[:,0], X_train[:,1], y_train, color='k', s=5)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_zlabel(r\"$\\varphi(x,y)$\")\n",
    "ax.set_title(\"Train dataset\")\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.plot_surface(xg, yg, phi, cmap=cm.jet, alpha=0.3)\n",
    "ax.scatter(X_test[:,0], X_test[:,1], y_test, color='k', s=5)\n",
    "\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_zlabel(r\"$\\varphi(x,y)$\")\n",
    "ax.set_title(\"Test dataset\")\n",
    "fig.savefig(\"train_test_radial_function.png\", dpi= 300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GA3GAzrkH6-k"
   },
   "source": [
    "## - Step 4 **[TODO]**: Define a 1-hidden layer $\\mathcal{MLP}$ $h_\\theta^{(1)}$ and a 2-hidden layers $\\mathcal{MLP}$ $h_\\theta^{(2)}$ for further comparisons, using the `torch.nn` package. Adopt an adequate number of of hidden neurons for both cases.\n",
    "\n",
    "**Note**: use $ReLU$ activation functions and linear output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1693410873967,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "_8Y-hKUgDKlE"
   },
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1693410873967,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "6eqk2UGjkvev"
   },
   "outputs": [],
   "source": [
    "'''[TODO] Define 1-hidden-layer MLP selecting an adequate fan_in'''\n",
    "fan_in = 10000\n",
    "h_theta1 = nn.Sequential(\n",
    " # [TODO]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1693410873967,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "PUaK3Bq-kyvn"
   },
   "outputs": [],
   "source": [
    "'''[TODO] Define 2-hidden-layer MLP'''\n",
    "fan_in = 100\n",
    "h_theta2 = nn.Sequential(\n",
    " # [TODO]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93WvyG4xI9T4"
   },
   "source": [
    "## - Step 5 **[TODO]**: Define the best optimizer for both $h_\\theta^{(1)}$ and $h_\\theta^{(2)}$. Choose the learning rate accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1693410873968,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "Cobc_jmUIoHw"
   },
   "outputs": [],
   "source": [
    "''' [TODO] Define the optimizer for 1-hidden-layer MLP'''\n",
    "learning_rate1 = 0.0001\n",
    "optimizer1 =  # [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1693410873968,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "RTzr2nHhHuf4"
   },
   "outputs": [],
   "source": [
    "''' [TODO] Define the optimizer for 2-hidden-layers MLP'''\n",
    "learning_rate2 = 0.0001\n",
    "optimizer2 =  # [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg96YTMlK7rf"
   },
   "source": [
    "## - Step 6 **[TODO]**: setup the gradient descent strategy and the loss function. Optional: initialize the weights according to the best strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1693410873968,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "eDzjZxEMk5C3"
   },
   "outputs": [],
   "source": [
    "''' [TODO] Gradient descent set up and loss function'''\n",
    "# Define the loss function\n",
    "loss_fn =  # [TODO]\n",
    "\n",
    "# number of epochs\n",
    "n_e = 1000\n",
    "\n",
    "# batch size and batch per epochs\n",
    "batch_size = 200\n",
    "batches_per_epoch = len(X_train) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HFP4U7cLr__"
   },
   "source": [
    "## - Step 7 **[TODO]**: Train $h_\\theta^{(1)}$ and $h_\\theta^{(2)}$ and track the approximation error for training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386129,
     "status": "ok",
     "timestamp": 1693411260035,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "7_q9Potwk7LP",
    "outputId": "0da3348b-b92c-4a96-b19b-87ce4159389c"
   },
   "outputs": [],
   "source": [
    "''' [TODO] Train 1-hidden-layer MLP'''\n",
    "import tqdm\n",
    "train_loss_hist1 = []\n",
    "test_loss_hist1 = []\n",
    "for epoch in range(n_e):\n",
    "    epoch_loss = []\n",
    "    # set model in training mode\n",
    "    h_theta1.train()\n",
    "\n",
    "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for i in bar:\n",
    "            # take a batch\n",
    "            start = i * batch_size\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # infer (forward)\n",
    "            y_pred =  # [TODO]\n",
    "            # compute the loss\n",
    "            loss =  # [TODO]\n",
    "            # reset previously saved gradients and empty the optimizer memory\n",
    "            optimizer1.zero_grad()\n",
    "            # run backward propagation\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer1.step()\n",
    "            # compute and store metrics\n",
    "            epoch_loss.append(float(loss))\n",
    "\n",
    "    # set model in evaluation mode to infer the class in the test set\n",
    "    # without storing gradients for brackprop\n",
    "    h_theta1.eval()\n",
    "    # infer the class over the test set\n",
    "    y_pred = h_theta1(X_test).squeeze()\n",
    "    acc = float(loss_fn(y_pred, y_test))\n",
    "    train_loss_hist1.append(np.mean(epoch_loss))\n",
    "    test_loss_hist1.append(acc)\n",
    "    print(f\"Epoch {epoch} validation: MSE={acc:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35395,
     "status": "ok",
     "timestamp": 1693411295370,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "7aESnHNzk9kn",
    "outputId": "43a1472b-4888-4ede-ba06-97c0b24f0dcc"
   },
   "outputs": [],
   "source": [
    "'''[TODO] Train 2-hidden-layer MLP'''\n",
    "import tqdm\n",
    "train_loss_hist2 = []\n",
    "test_loss_hist2 = []\n",
    "for epoch in range(n_e):\n",
    "    epoch_loss = []\n",
    "    # set model in training mode\n",
    "    h_theta2.train()\n",
    "\n",
    "    with tqdm.trange(batches_per_epoch, unit=\"batch\", mininterval=0) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for i in bar:\n",
    "            # take a batch\n",
    "            start = i * batch_size\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # infer (forward)\n",
    "            y_pred = h_theta2(X_batch).squeeze()\n",
    "            # compute the loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # reset previously saved gradients and empty the optimizer memory\n",
    "            # [TODO]\n",
    "            # run backward propagation\n",
    "            # [TODO]\n",
    "            # update weights\n",
    "            optimizer2.step()\n",
    "            # compute and store metrics\n",
    "            epoch_loss.append(float(loss))\n",
    "\n",
    "    # set model in evaluation mode to infer the class in the test set\n",
    "    # without storing gradients for brackprop\n",
    "    h_theta2.eval()\n",
    "    # infer the class over the test set\n",
    "    y_pred = h_theta2(X_test).squeeze()\n",
    "    acc = float(loss_fn(y_pred, y_test))\n",
    "    train_loss_hist2.append(np.mean(epoch_loss))\n",
    "    test_loss_hist2.append(acc)\n",
    "    # print(f\"Epoch {epoch} validation: Accuracy={acc:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 487799,
     "status": "ok",
     "timestamp": 1693411295370,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "_W1zPfRnk_tb"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.semilogy(train_loss_hist1,\n",
    "            color='b',\n",
    "            linewidth=1,\n",
    "            label=r\"train $h^{1}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
    "ax.semilogy(test_loss_hist1,\n",
    "            color='r',\n",
    "            linewidth=1,\n",
    "            label=r\"test $h^{1}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
    "ax.semilogy(train_loss_hist2,\n",
    "            color='steelblue',\n",
    "            linewidth=3,\n",
    "            label=r\"train $h^{2}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
    "ax.semilogy(test_loss_hist2,\n",
    "            color='orange',\n",
    "            linewidth=3,\n",
    "            label=r\"test $h^{2}_{\\mathbf{\\theta}}(\\mathbf{x})$\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$\\Vert \\varphi(x,y)-h_{\\mathbf{\\theta}}(\\mathbf{x}) \\Vert^2$\")\n",
    "ax.set_xlim(0,n_e)\n",
    "ax.set_ylim(1e-6,1e0)\n",
    "ax.legend(frameon=False, loc='upper center', bbox_to_anchor=(0.5, 1.0),ncol=2,)\n",
    "fig.savefig(\"mse_radial_compare.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 487442,
     "status": "ok",
     "timestamp": 1693411295371,
     "user": {
      "displayName": "Filippo Gatti",
      "userId": "07349036382870140489"
     },
     "user_tz": -120
    },
    "id": "7zWTAbXvlB3f"
   },
   "outputs": [],
   "source": [
    "h_theta2.eval()\n",
    "fig = plt.figure(figsize=(4,4),)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.plot_surface(xg, yg, phi, cmap=cm.jet, alpha=0.3)\n",
    "ax.scatter(X_train[:,0], X_train[:,1], y_train, color='k', s=30)\n",
    "ax.scatter(X_test[:,0], X_test[:,1], y_test, color='orange', s=30)\n",
    "ax.scatter(X_train[:,0], X_train[:,1],\n",
    "           h_theta2(X_train).detach().cpu().numpy(), color='k', s=30, marker=\"x\")\n",
    "ax.scatter(X_test[:,0], X_test[:,1],\n",
    "           h_theta2(X_test).detach().cpu().numpy(), color='orange', s=30, marker=\"x\")\n",
    "\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_zlabel(r\"$\\varphi(x,y)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSHEVb3oMN47"
   },
   "source": [
    "## - Step 8 **[TODO]**: what is the best traded-off between total number of weights, training time, approximation error?\n",
    "\n",
    "Try different solutions !"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
