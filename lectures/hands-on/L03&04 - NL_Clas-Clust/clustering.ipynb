{"cells":[{"cell_type":"code","metadata":{"cell_id":"aefc680201074d82a56eded82d037a03","deepnote_cell_type":"code"},"source":"","block_group":"670a1ca45cda4bf49f12e6cedc166abe","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"8256ccddc4ff472999ecb91f782bfba9","deepnote_cell_type":"markdown"},"source":"# Hands-on: Clustering","block_group":"d7f43aa0865c46308c23786c724ccd66"},{"cell_type":"markdown","metadata":{"cell_id":"1b4b2cfc8f7e452380ed12e2af9c15d4","deepnote_cell_type":"markdown"},"source":"Clustering is a particular class of Machine Learning tools called unsupervised machine learning, whose objective is to separate your data into homogeneous groups with common characteristics. \n\nIt is a technique that allows us to find groups of similar objects, objects that are more related to each other than to objects in other groups. Examples of business-oriented applications of clustering include the grouping of documents, music, and movies by different topics, or finding customers that share similar interests based on common purchase behaviors as a basis for recommendation engines.\n\nThe basic question is how do we measure similarity between objects? We can define similarity as the opposite of distance, and a commonly used distance for clustering samples with continuous features is the squared Euclidean distance between two points x and y in m-dimensional space:\n$$\nd^2(x,y) -\\sum_{j=1}^m(x_j-y_j)^2 = \\Vert x-y\\Vert^2_2  \n$$\n\nBased on this Euclidean distance metric, we can describe the k-means algorithm as a simple optimization problem using a an iterative process for minimizing the within-cluster Sum of Squared Errors (SSE), which is sometimes also called cluster inertia or distortion:\n\n$$\n\\mathrm{SSE} = \\sum_{i=1}^n \\sum_{j=1}^k w^{(i,j)}=1 \\Vert x^{(i)}-\\mu^{(j)}\\Vert^2_2\n$$\n\nwhere $\\mu^{(j)}$ is the centroid of the $j$-th cluster, and $w^{(i,j)}=1$ if $x^{(i)}$ is in cluster $j$ and $0$ otherwise.\n\nThe following four types are the most widely used types of clustering models.\n\n* **Centroid Models**: uses the distance between a data point and the centroid of the cluster to group data. K-means clustering is an example of a centroid model.\n* **Distribution Models**: segments data based on their probability of belonging to the same distribution. Gaussian Mixture Model (GMM) is a popular distribution model.\n* **Connectivity Models**: uses the closeness of the data points to decide the clusters. Hierarchical Clustering Model is a widely used connectivity model.\n* **Density Models**: scans the data space and assigns clusters based on the density of data points. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular density model.\n\nMore Information here: https://scikit-learn.org/stable/modules/clustering.html","block_group":"bb8b63411c0e49ba9da9d383608f51b1"},{"cell_type":"markdown","metadata":{"cell_id":"982776711a0f47769d68706106419392","deepnote_cell_type":"markdown"},"source":"## K-means","block_group":"1e909710517b4f8d84251af0f29c3c09"},{"cell_type":"markdown","metadata":{"cell_id":"f5fc2dea227a49a39fa8e3b58fc58019","deepnote_cell_type":"markdown"},"source":"The K-means algorithm is a very well known unsupervised algorithm in clustering. In this lab practice we will detail how it works and the useful ways to optimize it. \n\nThis algorithm was designed in 1957 at Bell Laboratories by Stuart P. Lloyd as a pulse code modulation (PCM) technique. It was presented to the general public only in 1982. In 1965 Edward W. Forgy had already published an almost similar algorithm, which is why K-means is often called the Lloyd-Forgy algorithm. \n\nThe fields of application are diverse: customer segmentation, data analysis, image segmentation, semi-supervised learning etc.","block_group":"39f700066dc04a58a0883edf33dcfc09"},{"cell_type":"markdown","metadata":{"cell_id":"9f2546db2bbf499894ccbf32a14a4820","deepnote_cell_type":"markdown"},"source":"### The principle of the k-means algorithm\n\nGiven points and an integer $k$, the algorithm aims to divide the points into $k$ homogeneous and compact groups, called clusters. Let's look at the example below:","block_group":"821552b80bb2423381a7e42d58f2342f"},{"cell_type":"code","metadata":{"cell_id":"badaab2ecab44716b25c69cb4a8566e8","deepnote_cell_type":"code"},"source":"#Just the basics \nimport numpy as np\nimport pandas as pd\n\n# Plotting te things\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import colors\n\n# Mathematical Analysis\nfrom scipy import linalg\nfrom scipy.spatial import Voronoi, voronoi_plot_2d \n\n# Metrics\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.neighbors import NearestCentroid\n\n# Dataset\nfrom sklearn import datasets\nfrom sklearn.datasets import make_blobs\n\n# Dimensionality reduction\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n# Modeling\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import DBSCAN\n","block_group":"228d351946284a7e9b73128586a0854c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"fec57ba472d849c5acfccad601454bdd","deepnote_cell_type":"markdown"},"source":"1. Let's create a dataset of size $N=2000$, distributed in three clusters with a Gaussian distribution,","block_group":"e7bc656c918144a8845fd3c2ecdd8cee"},{"cell_type":"code","metadata":{"cell_id":"1fb3eeb353f740398f313927827eecae","deepnote_cell_type":"code"},"source":"n_samples = 2000\nrandom_state = 130 # fix the random state for reproducibility\nn_components=3\nstd_dev=1.0 \n# This function makes some clusters with 2D coordinates in X and a label y\n# Usefull for testing unsupervised ML as well classification\n\nX,y = make_blobs(n_samples=n_samples, centers=n_components, cluster_std=std_dev, random_state=random_state)\n\nfigure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\nax1.scatter(X[:,0], X[:,1])\nax1.set_title ('Clusters',fontsize=20)\nax2.scatter(X[:,0], X[:,1], c = y)\nax2.set_title ('Classes',fontsize=20)\nxlim, ylim = ax1.get_xlim(),ax1.get_ylim()","block_group":"892a53efdd724e14aad1ccd8df3a3d60","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"8d8e13095e2449e6a701898a6c72a002","deepnote_cell_type":"markdown"},"source":"2. Create a model with $k=3$ as visually this should represent a good clustering of the data","block_group":"4de4cf0d598f4417a1a71ac53439d792"},{"cell_type":"code","metadata":{"cell_id":"8e365fedf1d4433394e016b2c12b46f5","deepnote_cell_type":"code"},"source":"# Fix the number of clusters that you want \nnb_c = 3\n\n# create a K-means model with that number of clusters  \nmodel = KMeans(n_clusters=nb_c\n               , init='random'\n               , n_init=10\n               , max_iter=300\n               , tol=1e-04\n               , random_state=random_state\n              )\n\n# cluster the data with that model \nmodel.fit(X)\n\n# here for the demonstration use the same data to see how it performs\n# y_pred :labels assigned to each data points\n# centers : centroid positions of each cluster\n\ny_pred = model.predict(X)\ncenters = model.cluster_centers_\n# Plot the data\n\nvor = Voronoi(centers) \nvoronoi_plot_2d(vor\n                , show_vertices=False\n                , line_colors='red'\n                , line_width=2\n                , line_alpha=0.6\n                , point_size=10) \n\nplt.scatter(X[:, 0], X[:,1], c = y_pred)\nplt.xlim(xlim)\nplt.ylim(ylim)\nplt.title ('Clusters',fontsize=20)","block_group":"e61081e72ed843b88af818d638c468e1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"c17a602aa57748e5a143434e100dcf61","deepnote_cell_type":"markdown"},"source":"**Note**: that the cluster labels can be different but this is not important, so seems to perform perfectly as can be seen from the initial 3 classes that have been built.\n\n**Interpretation**: k-means divides the space using hyperplanes to attibute a data point to a cluster. These planes can visualized in 2D by a Voronoï analysis. This will be more clear when increasing the requested number of clusters as bellow and when predict to which cluster a new data point is attributed.","block_group":"69338ce06ad6493db47d187ded976c3c"},{"cell_type":"markdown","metadata":{"cell_id":"f3f6805b6afb4a9ea253e316f94ea0cb","deepnote_cell_type":"markdown"},"source":"3. What if we change the wanted number of clusters ?","block_group":"57a86b215f8e4b49a582124246abe060"},{"cell_type":"code","metadata":{"cell_id":"727f8f0cd2964f45a8a151f2dc0f56db","deepnote_cell_type":"code"},"source":"# Fix the number of clusters that you want \ny_pred = np.zeros(n_samples) #create an empty array\nrandom_state = 130\nk_max = 10 \nnr = k_max//2\n\nmodels = []\n\nfigure, ax = plt.subplots(nrows=nr, ncols = 2, figsize=(14,5*nr))  \n\nfor nb_c in range(1,k_max+1):\n\n# create a K-means model with that number of clusters  \n    model = KMeans(n_clusters=nb_c\n               , init='random'\n               , n_init=10\n               , max_iter=300\n               , tol=1e-04\n               , random_state=random_state\n              )\n# cluster the data with that model \n    model.fit(X)\n\n# here for thedemonstration use the same data to see how it performs\n    y_pred = np.append(y_pred, model.predict(X))\n    models.append(model)\n\ny_p = np.reshape(y_pred,(k_max+1,n_samples))\n\nfor nb_c in range(1,k_max+1):\n    ax[(nb_c-1)//2,1-nb_c%2].scatter(X[:, 0], X[:,1], c = y_p[nb_c,:])\n    ax[(nb_c-1)//2,1-nb_c%2].set_title(f'number of clusters: {nb_c}', fontsize = 20)\n    ax[(nb_c-1)//2,1-nb_c%2].scatter(models[nb_c-1].cluster_centers_[:,0], models[nb_c-1].cluster_centers_[:,1], \n                marker='*', \n                color='red', \n                s=200);\n\n#     if nb_c>2:\n#         vor = Voronoi(models[nb_c-1].cluster_centers_) \n#         voronoi_plot_2d(vor, ax[(nb_c-1)//2,1-nb_c%2]\n#                 , show_vertices=False\n#                 , line_colors='red'\n#                 , line_width=2\n#                 , line_alpha=0.6\n#                 , point_size=10\n#                 , point_alpha=0\n#                 ) \n#     ax[(nb_c-1)//2,1-nb_c%2].set_xlim(xlim)\n#     ax[(nb_c-1)//2,1-nb_c%2].set_ylim(ylim)\n","block_group":"24f954d6ddf04822a4c088bb75d24ff9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"7c6946c9cdff414c96a0e27e1158bd7c","deepnote_cell_type":"markdown"},"source":"Except for $k=3$ one can see that the partitionning is inaccurate because the number of initial clusters is either lower or higher than the ideal number, in this case $3$ by construction. ","block_group":"eb41cf295acd45ceb82194564b05c237"},{"cell_type":"markdown","metadata":{"cell_id":"c17dd137151d4e12b4e01d2bceb8e569","deepnote_cell_type":"markdown"},"source":"3. Search for the optimal number of clusters\n\nThere are methods to determine the ideal number of clusters. \n\n####  Elbow method\nThe most common is the elbow method. \nIt is based on the notion of inertia. It is defined as follows: the sum of the Euclidean distances between each point and its associated centroid. Obviously, the higher the initial number of clusters, the more the inertia is reduced: the points have more chance to be next to a centroid. \n\nLet's look at what this gives on our example","block_group":"b5a5da4e736c4a92ad6f0aa55fa70187"},{"cell_type":"code","metadata":{"cell_id":"1c6d488127f44e00b4bdf1c5ff83c8c5","deepnote_cell_type":"code"},"source":"# Extract the inertia\n\nelb = []\nfor i in range(k_max):\n    elb.append(models[i].inertia_)\n    \n\nplt.plot(range(1,k_max+1),elb, marker = 'o')\nplt.xticks(range(1,k_max+1))\nplt.xlabel('Number of clusters in k-means', fontsize=16)\nplt.ylabel('Inertia or Distortion', fontsize=16)\nplt.show()","block_group":"b5875065572949a198f93e60f09b2456","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"0be50f98f45b4ada87a88abcabca795b","deepnote_cell_type":"markdown"},"source":"#### Silhouette coefficient\n\nOne can notice that the inertia stagnates after 3 clusters. This method is conclusive here. Nevertheless it can be coupled with a more precise approach but which requires more computing time: the silhouette coefficient, which is defined as follows\n\n$$\ns = \\frac{b-a}{\\max(a,b)}\n$$\n\nwhere $a$ is the average of the distances to the other observations of the same cluster (*i.e.* the intra-cluster average), and $b$ is the average distance to the nearest cluster. This coefficient can vary between $-1$ and $+1$. A $s$ coefficient close to $+1$ means that the observation is well located inside its own cluster, while a coefficient close to $0$ means that it is located near a border; finally, a coefficient close to $-1$ means that the observation is associated with the wrong cluster. \n\nThe calculation of this coefficient is included in the sklearn.metrics library.  \n\nAs for the inertia, it is judicious to display the evolution of the coefficient as a function of the number of clusters as shown below:","block_group":"564ef781d0be432d9c3676c9af6e9e8a"},{"cell_type":"code","metadata":{"cell_id":"4a25594fa62c49f4835f6f8a33273ec2","deepnote_cell_type":"code"},"source":"\n# Extract the silhouette\n\nsil = []\nfor i in range(2,k_max+1):\n    sil.append(silhouette_score(X, y_p[i]))\n\n    print(\n        \"For n_clusters =\",\n        i,\n        \"The average silhouette_score is :\",\n        sil[-1],\n    )\nplt.plot(range(2,k_max+1),sil, marker = 'o')\nplt.xticks(range(2,k_max+1))\nplt.xlabel('Number of clusters in k-means', fontsize=16)\nplt.ylabel('Silhouette coefficient', fontsize=16)\nplt.show()","block_group":"e946c5e737584b3299d42333a1d7c365","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"d2d2d55275514797949e0c561d1e2cce","deepnote_cell_type":"markdown"},"source":"4. Predict now to what cluster belong new points according to the best model \n\nIt is usefull to use the voronoi to visually check the performance of the clustering and see how it works","block_group":"e5e8550e1fa74bafbb1800576b93fb77"},{"cell_type":"code","metadata":{"cell_id":"b1df80b0e0c44d7cadbe0fb6bedd6271","deepnote_cell_type":"code"},"source":"# Select dim points randomly\nn = 100\ndim = 1\nnp.random.seed(42)\n\nx = np.array([])\nx = np.append(x,np.random.rand(n, dim)*14-10)\nx = np.append(x,np.random.rand(n, dim)*22-11)\nX_new = np.reshape(x,(2,n)).transpose()\n\n# Best model \nnb_c = 3\n\ny_p_new = models[nb_c-1].predict(X_new)\ncenters = models[nb_c-1].cluster_centers_\n\nfigure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n\nvor = Voronoi(centers) \n\nax1.scatter(X[:,0],X[:,1], c = y_p[nb_c])\nvoronoi_plot_2d(vor\n                 , ax1\n                 , show_vertices=False\n                 , line_colors='red'\n                 , line_width=2\n                 , line_alpha=0.6\n                 , point_size=10) \n\nax2.scatter(X_new[:,0],X_new[:,1], c = y_p_new)\nvoronoi_plot_2d(vor\n                 , ax2\n                 , show_vertices=False\n                 , line_colors='red'\n                 , line_width=2\n                 , line_alpha=0.6\n                 , point_size=10) \nax1.set_xlim(xlim)\nax1.set_ylim(ylim)\nax2.set_xlim(xlim)\nax2.set_ylim(ylim)\nplt.show()","block_group":"c55c09b4b1634205bc19170fbf07a7ee","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"33cd0ae21bd54403b5fc84a81a967cc1","deepnote_cell_type":"markdown"},"source":"#### Practice this on more tricky example","block_group":"c9b16c0312ac447e901342e24bb53d1d"},{"cell_type":"code","metadata":{"cell_id":"22e37261502c469e9a31b6582d6a869f","deepnote_cell_type":"code"},"source":"n_samples = 2000\nrandom_state = 0 # fix the random state for reproducibility\nn_components = 4\nstd_dev=1\n# This function makes some clusters with 2D coordinates in X and a label y\n# Usefull for testing unsupervised ML as well classification\n\nX,y = make_blobs(n_samples=n_samples, centers=n_components, cluster_std=std_dev, random_state=random_state)\n\nfigure, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14,5))\n\nax1.scatter(X[:,0], X[:,1])\nax1.set_title ('Clusters',fontsize=20)\nax2.scatter(X[:,0], X[:,1], c = y)\nax2.set_title ('Classes',fontsize=20)\nxlim, ylim = ax1.get_xlim(),ax1.get_ylim()\nplt.show()","block_group":"02d983ca57a44ce8974674ef76f41a6a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"d35e596f04a6491ca8a11df42ece42aa","deepnote_cell_type":"code"},"source":"k_max = 10 \nnr = k_max//2\n","block_group":"44154bfd797a44a397318ddff16e0ac0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"7504e06d1fc24286aa4e90dc032c23ad","deepnote_cell_type":"code"},"source":"# Extract the inertia\n","block_group":"b09bfaa33fbf4b2196bc4feb5b20e346","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"261c0f01b2114c5aacf638dfd91b0238","deepnote_cell_type":"markdown"},"source":"### Conclusion:\n\nHere K-means still perdorms quite well, even if the inertia and siloutte score are less clear. \n\n* Advantages: K-Means is fast and scalable\n\n* Drawbacks: The model performance is highly impacted by the initial centroids. Some centroids initiation can produce sub-optimal results. K-Means model does not perform well when the cluster sizes vary a lot, have different densities, or have a non-spherical shape.\n\n* Extentions: K-means++ which contains a more clever way to initialize the centroids, often used in the Gaussian Mixture Model.  ","block_group":"b34b3a75e7b24b27a0b6e238f3739345"},{"cell_type":"markdown","metadata":{"cell_id":"08c93bb15add4fbea30b7f506bda8671","deepnote_cell_type":"markdown"},"source":"## Gaussian Mixture Model","block_group":"d24d14ecfc8c417581302f17cedf776e"},{"cell_type":"markdown","metadata":{"cell_id":"6923c6bd2341444698727c54cc773cdd","deepnote_cell_type":"markdown"},"source":"Gaussian Mixture Model (GMM) is a probabilistic model that assumes each data point belongs to a Gaussian distribution. It uses the expectation-maximization (EM) algorithm.\n\nIn the expectation step, the algorithm estimates the probability of each data point belonging to each cluster.\nIn the maximization step, each cluster is updated using the estimated probability of belonging to the cluster of all the data points.\nThe updates of the cluster are mostly impacted by the data points with high probabilities of belonging to the cluster.\n\nThe Python code implementation of GMM is similar to the K-Means clustering model, we just need to change the method from KMeans to GaussianMixture.\n\nOne difference is that the value for n_init from the default value of $1$ is changed to $5$. n_init is the number of initialization to generate. When setting it to $5$, it means that $5$ initializations for the model will be performed, and the one with the best result is kept.","block_group":"fac9ddd59a3549a4b0c2e7d7e52b9e92"},{"cell_type":"code","metadata":{"cell_id":"d74cc4b0e4b14c468e597824eba108c0","deepnote_cell_type":"code"},"source":"k_max = 10 \nnr = k_max//2\n\nmodels_gmm = []\ny_pred = np.zeros(n_samples) #create an empty array\n\nfigure, ax = plt.subplots(nrows=nr, ncols = 2, figsize=(14,5*nr))  \n\nfor nb_c in range(1,k_max+1):\n\n# create a GMM model with that number of clusters  \n    model = GaussianMixture(n_components=nb_c\n                            , n_init= 5\n                            , random_state=random_state)\n    \n# cluster the data with that model \n    model.fit(X)\n\n# here for the demonstration use the same data to see how it performs\n    y_pred= np.append(y_pred, model.predict(X))\n    models_gmm.append(model)\n\ny_p_gmm  = np.reshape(y_pred,(k_max+1,n_samples))\n\nfor nb_c in range(1,k_max+1):\n    ax[(nb_c-1)//2,1-nb_c%2].scatter(X[:, 0], X[:,1], c = y_p_gmm[nb_c,:])\n    ax[(nb_c-1)//2,1-nb_c%2].set_title(f'number of clusters: {nb_c}', fontsize = 20)\n    ax[(nb_c-1)//2,1-nb_c%2].scatter(models_gmm[nb_c-1].means_[:,0], models_gmm[nb_c-1].means_[:,1], \n                marker='*', \n                color='red', \n                s=200);\n","block_group":"c2e5925baeb04273b26da510c057fe29","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"8cf5cdad94cf4592a10bda055dcae558","deepnote_cell_type":"code"},"source":"# Extract the log likelyhood\n\nelb = []\naic = []\nbic = []\nfor i in range(k_max):\n    elb.append(models_gmm[i].score(X))\n    aic.append(models_gmm[i].aic(X))\n    bic.append(models_gmm[i].bic(X))\n    \n# Extract the silhouette\nsil = []\nfor i in range(2,k_max+1):\n    sil.append(silhouette_score(X, y_p_gmm[i]))\n\n    print(\n        \"For n_clusters =\",\n        i,\n        \"The average silhouette_score is :\",\n        sil[-1],\n    )\n    \nfigure, axs = plt.subplots(nrows = 2, ncols=2, figsize=(14,10))\n\naxs[0,0].plot(range(1,k_max+1),elb, marker = 'o')\naxs[0,0].set_xticks(range(1,k_max+1))\naxs[0,0].set_xlabel('Number of clusters in gmm', fontsize=16)\naxs[0,0].set_ylabel('Log Likelyhood ', fontsize=16)\n\naxs[0,1].plot(range(2,k_max+1),sil, marker = 'o')\naxs[0,1].set_xticks(range(2,k_max+1))\naxs[0,1].set_xlabel('Number of clusters in gmm', fontsize=16)\naxs[0,1].set_ylabel('Silhouette coefficient', fontsize=16)\n\naxs[1,0].plot(range(1,k_max+1),aic, marker = 'o')\naxs[1,0].set_xticks(range(1,k_max+1))\naxs[1,0].set_xlabel('Number of clusters in gmm', fontsize=16)\naxs[1,0].set_ylabel('Akaike information Criterion', fontsize=16)\n\naxs[1,1].plot(range(1,k_max+1),bic, marker = 'o')\naxs[1,1].set_xticks(range(1,k_max+1))\naxs[1,1].set_xlabel('Number of clusters in gmm', fontsize=16)\naxs[1,1].set_ylabel('Bayesian Information Criterion', fontsize=16)\n\nplt.show()","block_group":"1f5a74b1983a4c13bde95d775665f3a9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"d1d85378d25549a482bc49175b6572a6","deepnote_cell_type":"markdown"},"source":"## Hierachical clustering","block_group":"36d24b6842f54de280d1a4f254f04b37"},{"cell_type":"markdown","metadata":{"cell_id":"7fc3cd3c31d44513ac0e46935bc7db6b","deepnote_cell_type":"markdown"},"source":"AgglomerativeClustering is a type of hierarchical clustering algorithm that will be used here as an example.\n\nIt uses a bottom-up approach and starts each data point as an individual cluster.\nThen the clusters that are closest to each other are connected until all the clusters are connected into one.\nThe hierarchical clustering algorithms produce a binary tree, where the root of the tree includes all the data points, and the leaves of the tree are the individual data points.\n\nThe Python code implementation of the hierarchical clustering model is similar to the K-Means clustering model, we just need to change the method from KMeans to AgglomerativeClustering","block_group":"b8e316063a4a451d822981fa2060d2a0"},{"cell_type":"code","metadata":{"cell_id":"74a93cd631264adba307e1eacd9c65c5","deepnote_cell_type":"code"},"source":"k_max = 10 \nnr = k_max//2\n\nmodels_hc = []\ny_pred = np.zeros(n_samples) #create an empty array\n\nfigure, ax = plt.subplots(nrows=nr, ncols = 2, figsize=(14,5*nr))  \n\nfor nb_c in range(1,k_max+1):\n\n# create a GMM model with that number of clusters  \n    model = AgglomerativeClustering(n_clusters=nb_c)\n    \n# cluster the data with that model \n    model.fit_predict(X)\n\n# here for the demonstration use the same data to see how it performs\n    y_pred= np.append(y_pred, model.fit_predict(X))\n    models_hc.append(model)\n\ny_p_hc  = np.reshape(y_pred,(k_max+1,n_samples))\n\n\n# for this method calculate the centroids externally\nclf = NearestCentroid()\n\nfor nb_c in range(1,k_max+1):\n    ax[(nb_c-1)//2,1-nb_c%2].scatter(X[:, 0], X[:,1], c = y_p_hc[nb_c,:])\n    ax[(nb_c-1)//2,1-nb_c%2].set_title(f'number of clusters: {nb_c}', fontsize = 20)\n    if nb_c>1:\n        clf.fit(X, y_p_hc[nb_c])\n        ax[(nb_c-1)//2,1-nb_c%2].scatter(clf.centroids_[:,0], clf.centroids_[:,1], \n                marker='*', \n                color='red', \n                s=200);\n","block_group":"a1d191aa15364c57851b21da460c422d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e5e4ab459a5940c5afed775aa743e1b0","deepnote_cell_type":"code"},"source":"# Extract the silhouette\nsil = []\nfor i in range(2,k_max+1):\n    sil.append(silhouette_score(X, y_p_hc[i]))\n\n    print(\n        \"For n_clusters =\",\n        i,\n        \"The average silhouette_score is :\",\n        sil[-1],\n    )\n    \nfigure, (ax) = plt.subplots(nrows=1, ncols=1, figsize=(7,5))\n\nax.plot(range(2,k_max+1),sil, marker = 'o')\nax.set_xticks(range(2,k_max+1))\nax.set_xlabel('Number of clusters in k-means', fontsize=16)\nax.set_ylabel('Silhouette coefficient', fontsize=16)\n\n\n\nplt.show()","block_group":"2bbf8a3f6cd740cf9c9f557fd0890e71","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"30a6f5b59b2e41f2963675c9b32e691f","deepnote_cell_type":"markdown"},"source":"## Density-based spatial clustering of applications with noise  (DBSCAN)","block_group":"28b4218fa5e74c8b8d11b57880856b18"},{"cell_type":"markdown","metadata":{"cell_id":"d1330e08d3a84e34995a03c8289764c7","deepnote_cell_type":"markdown"},"source":"DBSCAN defines clusters using data density. It has two important hyperparameters to tune, eps and min_samples.\n\n* eps is the epsilon distance to be considered as the neighborhood of a data point. It is the most important parameter for DBSCAN [6].\n* min_samples is the number of minimum data points in the neighborhood in order for a data point to be considered as a core data point. This number includes the data point itself [6].\n\nAll data points in the neighborhood of the core data points belong to the same cluster.\nThe data points that are not core data points and do not have a core data point in the neighborhood are considered outliers. The label -1 in the prediction results represents outliers. To learn more about anomaly detection.\n\n**An important point with respect to the other methods it that DBSCAN does not take a pre-defined number of clusters**, and it identifies the number of clusters based on the density distribution of the dataset. ","block_group":"ea23e01cd6a0476fb2f8db098b27a968"},{"cell_type":"code","metadata":{"cell_id":"f7bf52abeb914f6094ff73f73445a089","deepnote_cell_type":"code"},"source":"# First go back to the first sample\n\nX0,y0 = make_blobs(n_samples=n_samples, centers=3, cluster_std=1.0, random_state=130)\nmodel_db0 = DBSCAN(eps = 0.8, min_samples = 3)\ny_p_db0= model_db0.fit_predict(X0)\n\n\n# Then the more tricky sample \nmodel_db = DBSCAN(eps = 0.3, min_samples = 3)\n    \n# cluster the data with that model \ny_p_db= model_db.fit_predict(X)\n\nprint('Simple sample: ')\nlabels0 = model_db0.labels_\nn_clusters0 = len(set(labels0)) - (1 if -1 in labels else 0)\nn_noise0 = list(labels0).count(-1)\nprint(\"Estimated number of clusters: %d\" % n_clusters0)\nprint(\"Estimated number of noise points: %d\" % n_noise0)\nprint('\\n')\nprint('Tricky sample: ')\nlabels = model_db.labels_\nn_clusters = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise = list(labels).count(-1)\n\nprint(\"Estimated number of clusters: %d\" % n_clusters)\nprint(\"Estimated number of noise points: %d\" % n_noise)\n\nfigure, (ax1,ax2) = plt.subplots(nrows=1, ncols = 2, figsize=(14,5))  \n\nax1.scatter(X0[:, 0], X0[:,1], c = y_p_db0)\nax1.set_title(f'Estimated number of clusters: {n_clusters0}', fontsize = 20)\nax2.scatter(X[:, 0], X[:,1], c = y_p_db)\nax2.set_title(f'Estimated number of clusters: {n_clusters}', fontsize = 20)\n\nplt.show()","block_group":"1d654dd6f8674e21a9abd7b243df9b8a","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cell_id":"eef3a6fd8f2f4a91ae1e7e07e12092bc","deepnote_cell_type":"markdown"},"source":"### Analysis\n\nFor the first simple sample, DBSCAN identifies the correct number of clusters, at the condition that two parameters are correctly set. In that case the tuning is somewhat easy. Here the number of cluster is 4 out of which 1 has a label -1 corresponding to the noise. The latter tha to be reduced but beware this is balance between this noise and the  chosen neighborhood.\n\nFor the more tricky sample is seem that DBSCAN has some difficulty at first sight to identify the clusters that overlap, and a more deeper tuning should be done (with all hyperparameters).  \n\nDBSCAN Pros: works on datasets of any shape and identifies anomalies automatically.\n\nDBSCAN Cons: It does not work well for identifying the clusters that are not well separated. Different clusters in the dataset need to have similar densities, otherwise, the DBSCAN does not perform well.\n","block_group":"118f1d209a184f7e95253d122dc31cb2"},{"cell_type":"markdown","metadata":{"cell_id":"0de84bf0f33e465892a02e3c4ce631e2","deepnote_cell_type":"markdown"},"source":"## Compare the clustering approaches","block_group":"2d2b269c1ad84456b6c1a8734c2cf747"},{"cell_type":"markdown","metadata":{"cell_id":"42838eb353fc458181f90ae9e43811eb","deepnote_cell_type":"markdown"},"source":"The comparison is made first by comparing the centers the histogram of labels for visual purposes. A more quantitative approach consists in comparing the found classes with the ground truth labels as the are known here. If the ground truth is not knowns which is basically the case in any clustering experiment, the silhouette values can be used.\n\nVarious scores can be used (see Scikit-learn guide):\n* **Homogeneity**: metric of a cluster labeling given a ground truth. A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class. This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.\n* **Completeness**: A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster. This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way.This metric is not symmetric: switching label_true with label_pred will return the homogeneity_score which will be different in general.\n* **V-measure**: V-measure cluster labeling given a ground truth. This score is identical to normalized_mutual_info_score with the 'arithmetic' option for averaging. The V-measure is the harmonic mean between homogeneity and completeness: This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won’t change the score value in any way. This metric is furthermore symmetric: switching label_true with label_pred will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.\n* **Rand index**: The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings\n* **Adjusted Mutual Information (AMI)**: it is an adjustment of the Mutual Information (MI) score to account for chance. It accounts for the fact that the MI is generally higher for two clusterings with a larger number of clusters, regardless of whether there is actually more information shared.","block_group":"496f9eccd2754852b6b56c7966c28ebd"},{"cell_type":"code","metadata":{"cell_id":"8388bcefcec44fc5a93db652e2e7c3a7","deepnote_cell_type":"code"},"source":"n_bins = 10\nk_max = 10\nnr = k_max//2\nfigure, ax = plt.subplots(nrows=nr, ncols = 2, figsize=(14,5*nr))  \n\nfor nb_c in range(1,k_max+1):\n    ax[(nb_c-1)//2,1-nb_c%2].hist([y_p_km[nb_c],y_p_gmm[nb_c],y_p_hc[nb_c]], bins=n_bins)\n    ax[(nb_c-1)//2,1-nb_c%2].legend(['K-means','GMM','HC'])\n","block_group":"62d343dd6533423c8ff6bbd296ecfdbc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"a87655c98d9e45ce9dd6c79f06655a8b","deepnote_cell_type":"code"},"source":"# Compare the centroids \nnb_c = 4\nvor = Voronoi(models_km[nb_c-1].cluster_centers_)\nprint('------------')\nprint('CENTROIDS:')\nprint('------------')\nprint('k-means') \nprint(models_km[nb_c-1].cluster_centers_)\nprint('Gaussian Mixture Model') \nprint(models_gmm[nb_c-1].means_)\nprint('Hierarchical clustering') \nprint(clf.centroids_)\n\nprint('------------')\nprint('PERFORMANCE:')\nprint('------------')\nprint('k-means') \nprint(f\"Homogeneity: {metrics.homogeneity_score(y, y_p_km[4]):.3f}\")\nprint(f\"Completeness: {metrics.completeness_score(y, y_p_km[4]):.3f}\")\nprint(f\"V-measure: {metrics.v_measure_score(y, y_p_km[4]):.3f}\")\nprint(f\"Adjusted Rand Index: {metrics.adjusted_rand_score(y, y_p_km[4]):.3f}\")\nprint(\"Adjusted Mutual Information:\" f\" {metrics.adjusted_mutual_info_score(y, y_p_km[4]):.3f}\")\nprint(f\"Silhouette Coefficient: {metrics.silhouette_score(X, y_p_km[4]):.3f}\")\nprint('\\n')\n\nprint('Gaussian Mixture Model') \nprint(f\"Homogeneity: {metrics.homogeneity_score(y, y_p_gmm[4]):.3f}\")\nprint(f\"Completeness: {metrics.completeness_score(y, y_p_gmm[4]):.3f}\")\nprint(f\"V-measure: {metrics.v_measure_score(y, y_p_gmm[4]):.3f}\")\nprint(f\"Adjusted Rand Index: {metrics.adjusted_rand_score(y, y_p_gmm[4]):.3f}\")\nprint(\"Adjusted Mutual Information:\" f\" {metrics.adjusted_mutual_info_score(y, y_p_gmm[4]):.3f}\")\nprint(f\"Silhouette Coefficient: {metrics.silhouette_score(X, y_p_gmm[4]):.3f}\")\nprint('\\n')\n\nprint('Hierarchical clustering') \nprint(f\"Homogeneity: {metrics.homogeneity_score(y, y_p_hc[4]):.3f}\")\nprint(f\"Completeness: {metrics.completeness_score(y, y_p_hc[4]):.3f}\")\nprint(f\"V-measure: {metrics.v_measure_score(y, y_p_hc[4]):.3f}\")\nprint(f\"Adjusted Rand Index: {metrics.adjusted_rand_score(y, y_p_hc[4]):.3f}\")\nprint(\"Adjusted Mutual Information:\" f\" {metrics.adjusted_mutual_info_score(y, y_p_hc[4]):.3f}\")\nprint(f\"Silhouette Coefficient: {metrics.silhouette_score(X, y_p_hc[4]):.3f}\")\nprint('\\n')\n\nprint('DBSCAN') \nprint(f\"Homogeneity: {metrics.homogeneity_score(y, y_p_db):.3f}\")\nprint(f\"Completeness: {metrics.completeness_score(y, y_p_db):.3f}\")\nprint(f\"V-measure: {metrics.v_measure_score(y, y_p_db):.3f}\")\nprint(f\"Adjusted Rand Index: {metrics.adjusted_rand_score(y, y_p_db):.3f}\")\nprint(\"Adjusted Mutual Information:\" f\" {metrics.adjusted_mutual_info_score(y, y_p_db):.3f}\")\nprint(f\"Silhouette Coefficient: {metrics.silhouette_score(X, y_p_db):.3f}\")\n\n\nfigure, (ax,ax1) = plt.subplots(nrows = 1, ncols = 2, figsize = (14,5))\n\nax.set_title(f'number of clusters: {nb_c}', fontsize = 20)\nvoronoi_plot_2d(vor\n                 , ax\n                 , show_vertices=False\n                 , line_colors='red'\n                 , line_width=2\n                 , line_alpha=0.6\n                 , point_size=0) \n\nax.scatter(models_km[nb_c-1].cluster_centers_[:,0], models_km[nb_c-1].cluster_centers_[:,1], \n                marker='*', \n#                color='lue', \n                s=200,\n          label = 'k-means');\nax.scatter(models_gmm[nb_c-1].means_[:,0], models_gmm[nb_c-1].means_[:,1], \n                marker='*', \n#                color='lue', \n                s=200,\n          label = 'GMM');\nax.scatter(clf.centroids_[:,0], clf.centroids_[:,1], \n                marker='*', \n#                color='red', \n                s=200,\n          label = 'HC');\nax.scatter(X[:, 0], X[:,1], c = y, alpha = 0.1)\nax1.hist([y_p_km[nb_c],y_p_gmm[nb_c],y_p_hc[nb_c]], bins=n_bins)\nxlim1=ax1.get_xlim()\nax1.plot(xlim1,[n_samples/4,n_samples/4], linestyle = 'dashed')\nax.set_xlim(xlim)\nax.set_ylim(ylim)\nax.legend()","block_group":"2f275a91d30940e49aef06fe71871b4e","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fe3254e6-9d62-4c8c-aa95-7472e9779ff6' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"63517b0983524f3e98e65b8f015ed226","deepnote_execution_queue":[]}}